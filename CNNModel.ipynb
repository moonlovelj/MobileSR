{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43b65a3-2af4-4a52-84e8-8d0956a60c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import pytorch_ssim\n",
    "\n",
    "class ConvLutModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=16):\n",
    "        super(ConvLutModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size * 2)\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # Dropout 概率为 50%\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # 应用 Dropout\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def perceptual_loss(sr, hr, vgg):\n",
    "    sr_vgg_features = vgg(sr)\n",
    "    hr_vgg_features = vgg(hr)\n",
    "    loss = nn.functional.mse_loss(sr_vgg_features, hr_vgg_features)\n",
    "    return loss\n",
    "\n",
    "def ssim_loss(sr, hr):\n",
    "    return 1 - ssim(sr, hr)\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = pytorch_ssim.create_window(window_size, channel).to(img1.device)\n",
    "    padding = window_size // 2  # 将padding设置为整数\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average, padding)\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average, padding):\n",
    "    mu1 = nn.functional.conv2d(img1, window, padding=padding, groups=channel)\n",
    "    mu2 = nn.functional.conv2d(img2, window, padding=padding, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = nn.functional.conv2d(img1 * img1, window, padding=padding, groups=channel) - mu1_sq\n",
    "    sigma2_sq = nn.functional.conv2d(img2 * img2, window, padding=padding, groups=channel) - mu2_sq\n",
    "    sigma12 = nn.functional.conv2d(img1 * img2, window, padding=padding, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        vgg19 = models.vgg19()\n",
    "        self.features = nn.Sequential(*list(vgg19.features)[:35]).eval()\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
